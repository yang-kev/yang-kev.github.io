\documentclass[12pt,reqno]{amsart}


\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{times}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}%
\usepackage{comment}

\usepackage{xcolor}
%\usepackage{hyperref}
%\hypersetup{
   %linktoc=all,
    %colorlinks=true
    %citecolor=blue
%}
\usepackage{stmaryrd}
\usepackage{dsfont}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{ass}[theorem]{Assumption}
\newtheorem{notation}[theorem]{Notation}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{rem}{Remark}
\numberwithin{equation}{section}

\urlstyle{same}

\usepackage[top=1.3in, bottom=1.3in, left=1.3in, right=1.3in]{geometry}
\usepackage[scr]{rsfso}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{eucal}
\usepackage{enumitem}
\setlist{leftmargin=*}
\usepackage[integrals]{wasysym}


\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother


\newcommand\nc{\newcommand}
\nc{\E}{\mathbb{E}}
\nc{\R}{\mathbb R}
\nc{\C}{\mathbb C}
\nc{\Z}{\mathbb Z}
\nc{\wt}{\widetilde}
\nc{\rnc}{\renewcommand}
\nc{\e}{\varepsilon}
\nc{\grad}{\nabla}
%\nc{\fsp}{\fontdimen2\font=2.21pt}
\nc{\fsp}{}

\rnc{\t}{{t}}
\nc{\s}{{s}}
\nc{\x}{{x}}
\nc{\y}{{y}}
\nc{\w}{{w}}
\nc{\z}{{z}}
\rnc{\r}{{r}}
\rnc{\k}{{k}}
\rnc{\j}{{j}}
\nc{\m}{{m}}
\nc{\n}{{n}}
\rnc{\i}{{i}}
\nc{\p}{{p}}
\rnc{\textstyle}{{}}



%\rnc{\t}{\mathrm{t}}
%\nc{\s}{\mathrm{s}}
%\nc{\x}{\mathrm{x}}
%\nc{\y}{\mathrm{y}}
%\nc{\w}{\mathrm{w}}
%\nc{\z}{\mathrm{z}}
%\rnc{\r}{\mathrm{r}}
%\rnc{\k}{\mathrm{k}}
%\rnc{\j}{\mathrm{j}}
%\nc{\m}{\mathrm{m}}
%\nc{\n}{\mathrm{n}}
%\rnc{\i}{\mathrm{i}}
%\nc{\p}{\mathrm{p}}


\nc{\abbr}[1]{{\sc\lowercase{#1}}}

\rnc{\leq}{\leqslant}
\rnc{\geq}{\geqslant}
\rnc{\d}{\mathrm{d}}
\newenvironment{nouppercase}{%
  \let\uppercase\relax%
  \renewcommand{\uppercasenonmath}[1]{}}{}
\pagestyle{plain}




\title{\Large Math 154: Probability Theory, HW 5\vspace{-0.1cm}}





\usepackage{setspace}
\begin{document}
%\pdfrender{StrokeColor=gray,TextRenderingMode=2,LineWidth=0.01pt}
\setstretch{0.97}
\begin{nouppercase}
\maketitle
\end{nouppercase}
%%%
\section*{Due March 5, 2024 by 9am}
%%%
\emph{Remember, if you are stuck, take a look at the lemmas/theorems/examples from class, and see if anything looks familiar.}
%%%
\section{Some practice with martingales}
%%%
%%%
\subsection{Polya's urn}
%%%
This is perhaps the most important urn model in probability. An urn contains $r$ red and $g$ green balls, where $r,g>0$. A ball is drawn from the urn, its color is noted, it is returned to the urn, and another ball of the same color is also added to the urn. Let $R_{n}$ denote the number of red balls {\color{red}drawn} after $n$ draws.
%%%
\begin{enumerate}
\item Suppose $r=1$. Show that $Y_{n}=\frac{1+R_{n}}{n+r+g}$ for $n\geq0$ is a martingale with respect to the filtration generated by $(R_{n})_{n\geq1}$, and show that $\sup_{n\geq1}|Y_{n}|\leq C$ for some constant $C>0$.
\item Suppose $r,g=1$. Let $T$ be the number of turns that is needed to draw a green ball. Show that $\E\frac{1}{T+2}=\frac14$. (Justify the application of any theorem you may be using!)
\end{enumerate}
%%%
%%%
\subsection{Bernstein's inequality}\label{subsection:bernstein}
%%%
Suppose $X_{1},\ldots\sim\mathrm{Bern}(p)$ are i.i.d., and define $Y_{i}=X_{i}-p$ for $i=1,\ldots,N$. Prove that there exists a constant $C>0$ such that for any $\e>0$, we have 
%
\begin{align*}
\mathbb{P}\left[\left|\frac{1}{\sqrt{N}}\sum_{i=1}^{N}Y_{i}\right|\geq \e\right]\leq \exp\left[-C\e^{2}\right].
\end{align*}
%
In particular, even though the maximum value of $Y_{1}+\ldots+Y_{N}$ can grow linearly in $N$, it likes to stay around $\sqrt{N}$. (\emph{Hint}: the process $S_{N}=Y_{1}+\ldots+Y_{N}$ is a martingale with respect to the filtration generated by $(X_{n})_{n\geq1}$; check this!)
%%%
\subsection{Maximal version of Bernstein's inequality}
%%%
We have shown that the running sum of independent Bernoulli's has ``sub-Gaussian behavior" in Problem \ref{subsection:bernstein}. We will show something similar but for the ``maximal process".

Recall notation from Problem \ref{subsection:bernstein}. Define $X_{N}:=N^{-\frac12}\sup_{1\leq n\leq N}|Y_{1}+\ldots+Y_{n}|$.
%%%
\begin{enumerate}
\item Show that for any $p\geq2$, we have $\E|X_{N}|^{p}\leq\left(\frac{p}{p-1}\right)^{p}\E|N^{-\frac12}\sum_{i=1}^{N}Y_{i}|^{p}$.
\item Use Problem \ref{subsection:bernstein} and the previous part to show that for some constant $C>0$, we have 
%
\begin{align*}
\E|X_{N}|^{2p}\leq\left(\frac{2p}{2p-1}\right)^{2p}(2p-1)!!C^{p}
\end{align*}
%
for any integer $p\geq1$.
\item Use the previous part to show that there exists a constant $K>0$ such that for any $\e>0$, we have
%
\begin{align*}
\mathbb{P}\left[|X_{N}|\geq \e\right]\leq\exp[-K\e^{2}].
\end{align*}
%
{\color{red}(\emph{Hint}: see the end of the notes for week 5 for a helpful lemma.)}
\end{enumerate}
%%%
%%%
\subsection{Gambler's ruin for an unfair game}\label{subsection:gambler}
%%%
Let $\{X_{n}\}_{n\geq1}$ be independent $\mathrm{Bern}(p)$ random variables with $p\neq0,\frac12,1$. Define $S_{N}=S_{N-1}{\color{red}+(-1)^{1+X_{N}}}$ for $N\geq1$ and set $S_{0}=0$. 
%%%
\begin{enumerate}
\item Show that $M_{N}=\left(\frac{1-p}{p}\right)^{S_{N}}$ is a martingale with respect to the filtration generated by $(X_{n})_{n\geq1}$.
%\item Show that $\tilde{M}_{N}=S_{N}-N(2p-1)$ is a martingale with respect to the filtration generated by $(X_{n})_{n\geq1}$.
\item Let $\tau$ be the first positive integer such that $S_{\tau}=-a$ or $S_{\tau}=b$ for $a,b>0$ fixed. Compute $\mathbb{P}[S_{\tau}=-a]$ in terms of $a,b,p$.
%\item Compute $\E[\tau]$ in terms of $a,b,p$.
\end{enumerate}
%%%
%%%
\begin{comment}
%%%
\subsection{Trigonometry is very important for studying stochastic processes}
%%%
Keep the notation of Problem \ref{subsection:gambler}, but suppose now that $p=\frac12$. 
%%%
\begin{enumerate}
\item Define $Y_{N}=\frac{\cos\left[\lambda\left(S_{N}-\frac{b-a}{2}\right)\right]}{(\cos\lambda)^{N}}$, where $a,b>0$ (suppose that $\cos\lambda\neq0$). Show that $Y_{N}$ is a martingale with respect to the filtration generated by $(X_{n})_{n\geq1}$.
\item Let $\tau$ be the first time that $S_{\tau}=-	1$ or $S_{\tau}=1$. Show that 
%
\begin{align*}
\E\left[\frac{\cos[\lambda S_{\tau}]}{(\cos\lambda)^{\tau}}\right]=1.
\end{align*}
%
(\emph{Remember to check conditions of any theorem you may want use to make sure it applies!})
\item Deduce that $\E[(\cos\lambda)^{-\tau}]=(\cos\lambda)^{-1}$.
\end{enumerate}
%%%
\end{comment}
%%%
%%%
\subsection{The ``quadratic" process of a martingale, and the Ito martingale}
%%%
%%%
\begin{enumerate}
\item Suppose that $\{X_{n}\}_{n\geq1}$ are independent mean zero random variables with variances $\sigma_{i}^{2}=\E X_{i}^{2}$. Show that $Y_{N}:=\sum_{i=1}^{N}X_{i}^{2}-\sum_{i=1}^{N}\sigma_{i}^{2}$ with $Y_{0}=0$ is a martingale with respect to the filtration generated by $\{X_{n}\}_{n\geq1}$.
\item Suppose in addition that $X_{i}$ are i.i.d. $\mathrm{Bern}(\frac12)$, and define $W_{i}=(-1)^{1+X_{i}}$. For any function $f:\Z\to\R$, define its \emph{Laplacian} to be $\Delta f(x)=f(x+1)+f(x-1)-2f(x)$. Moreover, define $Z_{N}=W_{1}+\ldots+W_{N}$. Show that $f(Z_{N})-\sum_{i=1}^{N-1}\frac12\Delta f(Z_{i})$ is a martingale with respect to the filtration generated by $\{X_{n}\}_{n\geq1}$.
\end{enumerate}
%%%
%%%
\subsection{Gaussian tail probabilities implies Gaussian moments}
%%%
Suppose $X$ is a continuous random variable such that $\mathbb{P}[|X|\geq C]\leq\exp\{-KC^{2}\}$ for all $C>0$ ($K$ is just a fixed constant). {\color{red}We showed in class that $\E|X|^{2q}\leq C_{1}(2q-1)!!C_{2}^{q}$ for all $q\geq1$ and for some $C_{1},C_{2}>0$ implies $\mathbb{P}[|X|\geq C]\leq\exp\{-KC^{2}\}$ for some $K>0$. We now show the converse is true.}
%%%
\begin{enumerate}
\item Let $p$ be the pdf of $X$. Show
%
\begin{align*}
\int_{\R}x^{2q}p(x)dx&=2q\int_{0}^{\infty}x^{2q}p(x)dx+2q\int_{0}^{\infty}x^{2q}p(-x)dx\\
&=2q\int_{0}^{\infty}x^{2q-1}\left(\int_{x}^{\infty}p(u)du\right)dx+2q\int_{0}^{\infty}x^{2q-1}\left(\int_{x}^{\infty}p(-u)du\right)dx\\
&\leq 4q\int_{0}^{\infty}x^{2q-1}\mathbb{P}[|X|\geq x]dx\\
&\leq 4q\int_{0}^{\infty}x^{2q-1}\exp\{-Kx^{2}\}dx.
\end{align*}
%
(\emph{Hint}: integration-by-parts is your friend.)
\item Using $u$-substitution, show that $\E|X|^{2q}\leq 4qK^{-q}\int_{0}^{\infty}y^{2q-1}\exp\{-y^{2}\}dy$.
\item (Bonus, +2pt): Show that $\int_{0}^{\infty}y^{2q-1}\exp\{-y^{2}\}dy\leq C_{1}(2q-1)!!C_{2}^{q}$ for some constants $C_{1},C_{2}>0$.
\end{enumerate}
%%%




\end{document}
%%%