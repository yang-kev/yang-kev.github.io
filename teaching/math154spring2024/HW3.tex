\documentclass[12pt,reqno]{amsart}


\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{times}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}%

\usepackage{xcolor}
%\usepackage{hyperref}
%\hypersetup{
   %linktoc=all,
    %colorlinks=true
    %citecolor=blue
%}
\usepackage{stmaryrd}
\usepackage{dsfont}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{ass}[theorem]{Assumption}
\newtheorem{notation}[theorem]{Notation}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{rem}{Remark}
\numberwithin{equation}{section}

\urlstyle{same}

\usepackage[top=1.3in, bottom=1.3in, left=1.3in, right=1.3in]{geometry}
\usepackage[scr]{rsfso}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{eucal}
\usepackage{enumitem}
\setlist{leftmargin=*}
\usepackage[integrals]{wasysym}


\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother


\newcommand\nc{\newcommand}
\nc{\E}{\mathbb{E}}
\nc{\R}{\mathbb R}
\nc{\C}{\mathbb C}
\nc{\Z}{\mathbb Z}
\nc{\wt}{\widetilde}
\nc{\rnc}{\renewcommand}
\nc{\e}{\varepsilon}
\nc{\grad}{\nabla}
%\nc{\fsp}{\fontdimen2\font=2.21pt}
\nc{\fsp}{}

\rnc{\t}{{t}}
\nc{\s}{{s}}
\nc{\x}{{x}}
\nc{\y}{{y}}
\nc{\w}{{w}}
\nc{\z}{{z}}
\rnc{\r}{{r}}
\rnc{\k}{{k}}
\rnc{\j}{{j}}
\nc{\m}{{m}}
\nc{\n}{{n}}
\rnc{\i}{{i}}
\nc{\p}{{p}}
\rnc{\textstyle}{{}}



%\rnc{\t}{\mathrm{t}}
%\nc{\s}{\mathrm{s}}
%\nc{\x}{\mathrm{x}}
%\nc{\y}{\mathrm{y}}
%\nc{\w}{\mathrm{w}}
%\nc{\z}{\mathrm{z}}
%\rnc{\r}{\mathrm{r}}
%\rnc{\k}{\mathrm{k}}
%\rnc{\j}{\mathrm{j}}
%\nc{\m}{\mathrm{m}}
%\nc{\n}{\mathrm{n}}
%\rnc{\i}{\mathrm{i}}
%\nc{\p}{\mathrm{p}}


\nc{\abbr}[1]{{\sc\lowercase{#1}}}

\rnc{\leq}{\leqslant}
\rnc{\geq}{\geqslant}
\rnc{\d}{\mathrm{d}}
\newenvironment{nouppercase}{%
  \let\uppercase\relax%
  \renewcommand{\uppercasenonmath}[1]{}}{}
\pagestyle{plain}




\title{\Large Math 154: Probability Theory, HW 3\vspace{-0.1cm}}





\usepackage{setspace}
\begin{document}
%\pdfrender{StrokeColor=gray,TextRenderingMode=2,LineWidth=0.01pt}
\setstretch{0.97}
\begin{nouppercase}
\maketitle
\end{nouppercase}
%%%
\section*{Due Feb 3, 2024 by 9am}
%%%
\emph{Remember, if you are stuck, take a look at the lemmas/theorems/examples from class, and see if anything looks familiar.}
%%%
\section{All of these problems require at least a little thought}
%%%
%%%
\subsection{Some magic in the Gaussian}
%%%
Suppose $X\sim N(0,1)$. 
%%%
\begin{enumerate}
\item Show that 
%
\begin{align*}
xe^{-\frac{x^{2}}{2}}=-\frac{d}{dx}e^{-\frac{x^{2}}{2}}
\end{align*}
%
\item Take any smooth function $f:\R\to\R$. Show that 
%
\begin{align*}
\E Xf(X)=\E f'(X),
\end{align*}
%
provided that both sides converge absolutely (when written as integrals). This is often known as \emph{Gaussian integration by parts}. (\emph{Hint}: the hint is in the name.)
\item Show that for any integer $k\geq0$, we have $\E X^{2k+1}=0$. 
\item Show that for any integer $k\geq0$, we have $\E X^{2k}=(2k-1)!!$, where $(2k-1)!!:=(2k-1)(2k-3)\ldots1$. (\emph{Hint}: use part (2) with $f(X)=X^{2k-1}$, and induct on $k$.)
\end{enumerate}
%%%
%%%
\subsection{Another fact about the Gaussian distribution}
%%%
Let $X\sim N(0,\sigma^{2})$ for some $\sigma>0$. Take any $\lambda\in\R$. Show that 
%
\begin{align}
\E e^{\lambda X}=e^{\frac{\lambda^{2}\sigma^{2}}{2}}.\nonumber
\end{align}
%
(\emph{Hint}: you may want to use the completing-the-square formula $a^{2}-2ba=(a-b)^{2}-b^{2}$ after you write out what the expectation on the LHS is as an integral on $\R$.) Give another proof of $\E X=0$ and $\E X^{2}=\sigma^{2}$ by differentiating both sides of this identity (once and twice) and setting $\lambda=0$.
%%%
\subsection{How does one sample from a distribution?}
%%%
Suppose $X$ is a continuous random variable, so that $\mathbb{P}(X\leq x)=\int_{-\infty}^{x}p(u)du$. Suppose $p$ is smooth and $p(u)>0$ for all $u\in\R$.
%%%
\begin{enumerate}
\item Show that the distribution of the random variable 
%
\begin{align*}
F(X)=\int_{-\infty}^{X}p(u)du
\end{align*}
%
is the uniform distribution on $[0,1]$. (Here, we evaluate the top limit of the integral at the random variable $X$. \emph{Hint}: it is not important to know what its inverse exactly is.)
\item Show that the random variable $-\log F(X)$ has p.d.f given by $e^{-x}$.
\end{enumerate}
%%%
\subsection{What?}
%%%
Suppose $X$ is an exponential random variable (i.e. it has the exponential distribution). Show that $\mathbb{P}(X>s+x|X>s)=\mathbb{P}(X>x)$ for any $x,s\geq0$. (\emph{Hint}: you can use the fact that the only function $g(\cdot)$ which satisfies $g(s+t)=g(s)g(t)$ for $s,t\geq0$ and $g(0)=1$ has the form $g(s)=e^{\mu s}$ for some $\mu\in\R$.)
%%%
\subsection{To the right or to the left?}
%%%
Let $X$ have variance $\sigma^{2}$, and write $m_{k}=\E X^{k}$. Define the \emph{skewness} of (the distribution of) $X$ to be $\mathrm{skw}(X)=\frac{\E(X-m_{1})^{3}}{\sigma^{3}}$. (This measures how much to the left/right the graph of the pdf is.)
%%%
\begin{enumerate}
\item Show that $\mathrm{skw}(X)=\frac{m_{3}-3m_{1}m_{2}+2m_{1}^{3}}{\sigma^{3}}$
\item Let $X_{1},\ldots,X_{n}$ be i.i.d. copies of $X$ (i.e. they are independent and have the same distribution). Set $S_{n}=X_{1}+\ldots+X_{n}$. Using that $\E[\prod_{i=1}^{n}f_{i}(W_{i})]=\prod_{i=1}^{n}\E[f_{i}(W_{i})]$ for any functions $f_{1},\ldots,f_{n}$ and any independent random variables $W_{1},\ldots,W_{n}$, show that $\mathrm{skw}(S_{n})=\frac{\mathrm{skw}(X_{1})}{\sqrt{n}}$.
\item Suppose $X\sim\mathrm{Bern}(n,p)$. Show that $\mathrm{skw}(X)=\frac{1-2p}{\sqrt{p(1-p)}}$.
\item Suppose $X\sim\mathrm{Bin}(n,p)$. Show that $\mathrm{skw}(X)=\frac{1-2p}{\sqrt{np(1-p)}}$.
\end{enumerate}
%%%
%%%
\subsection{Some more computations}
%%%
Keep the notation in the setting of Problem 1.5. Define the \emph{kurtosis} of $X$ by $\mathrm{kur}(X)=\frac{\E(X-m_{1})^{4}}{\sigma^{4}}$. (This is kind of like a variance, but it tells you a little more about the shape of the graph of the pdf.)
%%%
\begin{enumerate}
\item Show that if $X\sim N(\mu,\sigma^{2})$, then $\mathrm{kur}(X)=3$. Notice how this is much simpler! (It does not depend on the parameters of the distribution.)
\item Let $X_{1},\ldots,X_{n}$ be i.i.d. copies of $X$. Define $S_{n}=X_{1}+\ldots+X_{n}$. Show that $\mathrm{kur}(S_{n})=3+\frac{\mathrm{kur}(X_{1})-3}{n}$.
\end{enumerate}
%%%





















\end{document}
%%%