\documentclass[12pt,reqno]{amsart}


\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{times}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}%
\usepackage{comment}

\usepackage{xcolor}
%\usepackage{hyperref}
%\hypersetup{
   %linktoc=all,
    %colorlinks=true
    %citecolor=blue
%}
\usepackage{stmaryrd}
\usepackage{dsfont}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{ass}[theorem]{Assumption}
\newtheorem{notation}[theorem]{Notation}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{construction}[theorem]{Construction}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{rem}{Remark}
\numberwithin{equation}{section}

\urlstyle{same}

\usepackage[top=1.3in, bottom=1.3in, left=1.3in, right=1.3in]{geometry}
\usepackage[scr]{rsfso}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{eucal}
\usepackage{enumitem}
\setlist{leftmargin=*}
\usepackage[integrals]{wasysym}


\makeatletter
\newsavebox{\@brx}
\newcommand{\llangle}[1][]{\savebox{\@brx}{\(\m@th{#1\langle}\)}%
  \mathopen{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\newcommand{\rrangle}[1][]{\savebox{\@brx}{\(\m@th{#1\rangle}\)}%
  \mathclose{\copy\@brx\kern-0.5\wd\@brx\usebox{\@brx}}}
\makeatother


\newcommand\nc{\newcommand}
\nc{\E}{\mathbb{E}}
\nc{\R}{\mathbb R}
\nc{\C}{\mathbb C}
\nc{\Z}{\mathbb Z}
\nc{\wt}{\widetilde}
\nc{\rnc}{\renewcommand}
\nc{\e}{\varepsilon}
\nc{\grad}{\nabla}
%\nc{\fsp}{\fontdimen2\font=2.21pt}
\nc{\fsp}{}

\rnc{\t}{{t}}
\nc{\s}{{s}}
\nc{\x}{{x}}
\nc{\y}{{y}}
\nc{\w}{{w}}
\nc{\z}{{z}}
\rnc{\r}{{r}}
\rnc{\k}{{k}}
\rnc{\j}{{j}}
\nc{\m}{{m}}
\nc{\n}{{n}}
\rnc{\i}{{i}}
\nc{\p}{{p}}
\rnc{\textstyle}{{}}



%\rnc{\t}{\mathrm{t}}
%\nc{\s}{\mathrm{s}}
%\nc{\x}{\mathrm{x}}
%\nc{\y}{\mathrm{y}}
%\nc{\w}{\mathrm{w}}
%\nc{\z}{\mathrm{z}}
%\rnc{\r}{\mathrm{r}}
%\rnc{\k}{\mathrm{k}}
%\rnc{\j}{\mathrm{j}}
%\nc{\m}{\mathrm{m}}
%\nc{\n}{\mathrm{n}}
%\rnc{\i}{\mathrm{i}}
%\nc{\p}{\mathrm{p}}


\nc{\abbr}[1]{{\sc\lowercase{#1}}}

\rnc{\leq}{\leqslant}
\rnc{\geq}{\geqslant}
\rnc{\d}{\mathrm{d}}
\newenvironment{nouppercase}{%
  \let\uppercase\relax%
  \renewcommand{\uppercasenonmath}[1]{}}{}
\pagestyle{plain}




\title{\Large Math 154: Probability Theory, HW 6\vspace{-0.1cm}}





\usepackage{setspace}
\begin{document}
%\pdfrender{StrokeColor=gray,TextRenderingMode=2,LineWidth=0.01pt}
\setstretch{0.97}
\begin{nouppercase}
\maketitle
\end{nouppercase}
%%%
\section*{Due March 5, 2024 by 9am}
%%%
\emph{Remember, if you are stuck, take a look at the lemmas/theorems/examples from class, and see if anything looks familiar.}
%%%
\section{Trying to put everything into the lens of a martingale}
%%%
%%%
\subsection{An alternative characterization of conditional expectation}
%%%
Take $X_{1},\ldots,X_{N},Y$ a set of random variables. Let $f:\R^{N}\to\R$ be any continuous function. Show that 
%
\begin{align*}
\E[f(X_{1},\ldots,X_{N})\cdot Y]&=\E\{\E[f(X_{1},\ldots,X_{N})\cdot Y|X_{1},\ldots,X_{N}]\}\\
&=\E\{f(X_{1},\ldots,X_{N})\E[Y|X_{1},\ldots,X_{N}]\}.
\end{align*}
%
It turns out that $\E[Y|X_{1},\ldots,X_{N}]$ is the only random variable which depends only on $X_{1},\ldots,X_{N}$ for which this is true for all continuous $f:\R^{N}\to\R$. Hence, this is another definition of conditional expectation.
%%%
\subsection{Law of large numbers, martingale style}
%%%
It turns out independence is not crucial for the law of large numbers to hold, and that a martingale is really the underlying structure in a lot of cases. Let us see why.

Let $(M_{N})_{N\geq0}$ be a martingale with respect to the filtration generated by some sequence $(X_{n})_{n\geq0}$. We will assume $\sup_{N\geq0}\E|M_{N+1}-M_{N}|^{2}<\infty$ and $M_{0}=0$.
%%%
\begin{enumerate}
\item Using $M_{N}=\sum_{k=0}^{N-1}(M_{k+1}-M_{k})$, show that 
%
\begin{align*}
\E|M_{N}|^{2}&=\sum_{k=0}^{N-1}\E|M_{k+1}-M_{k}|^{2}\leq CN
\end{align*}
%
for some constant $C>0$. (\emph{Hint}: it may help to show that if $j<k$, then 
%
\begin{align*}
\E[(M_{k+1}-M_{k})(M_{j+1}-M_{j})]=\E\{(M_{j+1}-M_{j})\E[M_{k+1}-M_{k}|X_{1},\ldots,X_{k}]\}=0.
\end{align*}
%
To show this, it may help to use Problem 1.1 and the martingale property.)
\item Show that $\mathbb{P}[|N^{-1}M_{N}|\geq\e]\leq CN^{-1}\e^{-2}$ for any $\e>0$ and for some constant $C>0$. (\emph{Hint}: how does one control the tail probability using a second moment?)
\item Suppose now that $X_{n}$ are mean $0$ and variance $1$. Define $Y_{N}=\sum_{n=1}^{N}X_{n}$ and $Y_{0}=0$. Show that $\mathbb{P}[|N^{-1}Y_{N}|\geq \e]\leq CN^{-1}\e^{-2}$ for some constant $C>0$. (This is the law of large numbers as classically stated, e.g. as in class.)
\item There is no need to get this right or wrong; you will be given credit for any type of guess. Suppose that $\E|M_{N+1}-M_{N}|^{2}=1$ for every $N\geq0$. What do you think the distribution of $N^{-1/2}M_{N}$ converges to as $N\to\infty$? (We never defined what it meant for a distribution to converge, so use an intuitive ``definition".) 
\end{enumerate}
%%%




\end{document}
%%%